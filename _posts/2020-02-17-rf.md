---
title: "Decision Tree and Random Forrest - Including parameter tuning"
date: 2020-02-12
tags: [machine-learning]
header:
    image: "/images/tea.jpg"
excerpt: "Using a decision tree and random forrest to classify data. Furthermore we will take a look on how to tune parameters."
mathjax: "true"
---

# Abstract

The used dataset contains information about people and is mainly used for KNN. We will instead use a `decision tree` and and a `random forrest classifier` to distinguish between peeople who earn more or less than 50k per year. Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))

An individual’s annual income results from various factors. Intuitively, it is influenced by the individual’s education level, age, gender, occupation, and etc.

**Fields:**  
The dataset contains 16 columns

* Target filed: Income
    The income is divide into two classes: 50K
* Number of attributes: 14
    These are the demographics and other features to describe a person

The dataset can be found either on the UCI Machine Learning repository [Link](https://archive.ics.uci.edu/ml/datasets/Adult) or on Kaggle [Link](https://www.kaggle.com/wenruliu/adult-income-dataset).

In this articel we will also do some data exploration but this will not be the focus. The main goal is to understand how we can use an `random forrest classifier` and how it works.

<!-- TOC -->autoauto- [Abstract](#abstract)auto- [Decision Tree](#decision-tree)auto- [Random Forrest](#random-forrest)autoauto<!-- /TOC -->

# Decision Tree

Firstly we will take time to deeply understand why `decision trees` and `random forrest` do what they do and how we can tune them. As both of them have been explained on 28383 websites and books before I will just quote the explanation I thinks is the most understanable. To see the full version of it visit the following *towardsdatascience* [article](https://towardsdatascience.com/understanding-random-forest-58381e0602d2). Thanks to Tony Yiu for this awesome explanation.

Let’s quickly go over decision trees as they are the building blocks of the random forest model. Fortunately, they are pretty intuitive. I’d be willing to bet that most people have used a decision tree, knowingly or not, at some point in their lives.

**Simple Decision Tree Example**

It’s probably much easier to understand how a decision tree works through an example.
Imagine that our dataset consists of the numbers at the top of the figure to the left. We have two 1s and five 0s (1s and 0s are our classes) and desire to separate the classes using their features. The features are color (red vs. blue) and whether the observation is underlined or not. So how can we do this?

![Decision Tree Sructure](/images/dt-rf/dt1.jpeg)

Color seems like a pretty obvious feature to split by as all but one of the 0s are blue. So we can use the question, “Is it red?” to split our first node. You can think of a node in a tree as the point where the path splits into two — observations that meet the criteria go down the Yes branch and ones that don’t go down the No branch.
The No branch (the blues) is all 0s now so we are done there, but our Yes branch can still be split further. Now we can use the second feature and ask, “Is it underlined?” to make a second split.

The two 1s that are underlined go down the Yes subbranch and the 0 that is not underlined goes down the right subbranch and we are all done. Our decision tree was able to use the two features to split up the data perfectly. Victory!
Obviously in real life our data will not be this clean but the logic that a decision tree employs remains the same. At each node, it will ask —

>What feature will allow me to split the observations at hand in a way that the resulting groups are as different from each other as possible (and the members of each resulting subgroup are as similar to each other as possible)?

# Random Forrest





