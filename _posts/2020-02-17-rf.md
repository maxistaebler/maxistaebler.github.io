---
title: "Decision Tree and Random Forrest - Including parameter tuning"
date: 2020-02-12
tags: [machine-learning]
header:
    image: "/images/tea.jpg"
excerpt: "Using a decision tree and random forrest to classify data. Furthermore we will take a look on how to tune parameters."
mathjax: "true"
---

<!-- TOC -->

- [Abstract](#abstract)
- [Decision Tree](#decision-tree)
- [Random Forrest](#random-forrest)
- [Create some Cooooooode](#create-some-cooooooode)

<!-- /TOC -->

# 1. Abstract

The used dataset contains information about people and is mainly used for KNN. We will instead use a `decision tree` and and a `random forrest classifier` to distinguish between peeople who earn more or less than 50k per year. Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))

An individual’s annual income results from various factors. Intuitively, it is influenced by the individual’s education level, age, gender, occupation, and etc.

**Fields:**  
The dataset contains 16 columns

* Target filed: Income
    The income is divide into two classes: 50K
* Number of attributes: 14
    These are the demographics and other features to describe a person

The dataset can be found either on the UCI Machine Learning repository [Link](https://archive.ics.uci.edu/ml/datasets/Adult) or on Kaggle [Link](https://www.kaggle.com/wenruliu/adult-income-dataset).

In this articel we will also do some data exploration but this will not be the focus. The main goal is to understand how we can use an `random forrest classifier` and how it works.

# 2. Decision Tree

Firstly we will take time to deeply understand why `decision trees` and `random forrest` do what they do and how we can tune them. As both of them have been explained on 28383 websites and books before I will just quote the explanation I thinks is the most understanable. To see the full version of it visit the following *towardsdatascience* [article](https://towardsdatascience.com/understanding-random-forest-58381e0602d2). Thanks to Tony Yiu for this awesome explanation.

Let’s quickly go over decision trees as they are the building blocks of the random forest model. Fortunately, they are pretty intuitive. I’d be willing to bet that most people have used a decision tree, knowingly or not, at some point in their lives.

**Simple Decision Tree Example**

It’s probably much easier to understand how a decision tree works through an example.
Imagine that our dataset consists of the numbers at the top of the figure to the left. We have two 1s and five 0s (1s and 0s are our classes) and desire to separate the classes using their features. The features are color (red vs. blue) and whether the observation is underlined or not. So how can we do this?

![Decision Tree Sructure](/images/dt-rf/dt1.jpeg)

Color seems like a pretty obvious feature to split by as all but one of the 0s are blue. So we can use the question, “Is it red?” to split our first node. You can think of a node in a tree as the point where the path splits into two — observations that meet the criteria go down the Yes branch and ones that don’t go down the No branch.
The No branch (the blues) is all 0s now so we are done there, but our Yes branch can still be split further. Now we can use the second feature and ask, “Is it underlined?” to make a second split.

The two 1s that are underlined go down the Yes subbranch and the 0 that is not underlined goes down the right subbranch and we are all done. Our decision tree was able to use the two features to split up the data perfectly. Victory!
Obviously in real life our data will not be this clean but the logic that a decision tree employs remains the same. At each node, it will ask —

>What feature will allow me to split the observations at hand in a way that the resulting groups are as different from each other as possible (and the members of each resulting subgroup are as similar to each other as possible)?

# 3. Random Forrest

Random forest, like its name implies, consists of a large number of individual decision trees that operate as an [ensemble](https://en.wikipedia.org/wiki/Ensemble_learning). Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction (see figure below).

![Visualization of a Random Forest Model Making a Prediction](/images/dt-rf/rf1.jpeg)

**Visualization of a Random Forest Model Making a Prediction**

The fundamental concept behind random forest is a simple but powerful one — the wisdom of crowds. In data science speak, the reason that the random forest model works so well is:

>A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.

The low correlation between models is the key. Just like how investments with low correlations (like stocks and bonds) come together to form a portfolio that is greater than the sum of its parts, uncorrelated models can produce ensemble predictions that are more accurate than any of the individual predictions. **The reason for this wonderful effect is that the trees protect each other from their individual errors (as long as they don’t constantly all err in the same direction)**. While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction. So the prerequisites for random forest to perform well are:

* There needs to be some actual signal in our features so that models built using those features do better than random guessing.
* The predictions (and therefore the errors) made by the individual trees need to have low correlations with each other.

**Ensuring that the Models Diversify Each Other:**

So how does random forest ensure that the behavior of each individual tree is not too correlated with the behavior of any of the other trees in the model? It uses the following two methods:
Bagging (Bootstrap Aggregation) — Decisions trees are very sensitive to the data they are trained on — small changes to the training set can result in significantly different tree structures. Random forest takes advantage of this by allowing each individual tree to randomly sample from the dataset with replacement, resulting in different trees. This process is known as bagging.

Notice that with bagging we are not subsetting the training data into smaller chunks and training each tree on a different chunk. Rather, if we have a sample of size N, we are still feeding each tree a training set of size N (unless specified otherwise). But instead of the original training data, we take a random sample of size N with replacement. For example, if our training data was [1, 2, 3, 4, 5, 6] then we might give one of our trees the following list [1, 2, 2, 3, 6, 6]. Notice that both lists are of length six and that “2” and “6” are both repeated in the randomly selected training data we give to our tree (because we sample with replacement).

![Node splitting in a random forest model is based on a random subset of features for each tree.](/images/dt-rf/bagging.jpeg)

**Feature Randomness** — In a normal decision tree, when it is time to split a node, we consider every possible feature and pick the one that produces the most separation between the observations in the left node vs. those in the right node. In contrast, each tree in a random forest can pick only from a random subset of features. This forces even more variation amongst the trees in the model and ultimately results in lower correlation across trees and more diversification.
Let’s go through a visual example — in the picture above, the traditional decision tree (in blue) can select from all four features when deciding how to split the node. It decides to go with Feature 1 (black and underlined) as it splits the data into groups that are as separated as possible.

Now let’s take a look at our random forest. We will just examine two of the forest’s trees in this example. When we check out random forest Tree 1, we find that it it can only consider Features 2 and 3 (selected randomly) for its node splitting decision. We know from our traditional decision tree (in blue) that Feature 1 is the best feature for splitting, but Tree 1 cannot see Feature 1 so it is forced to go with Feature 2 (black and underlined). Tree 2, on the other hand, can only see Features 1 and 3 so it is able to pick Feature 1.

>So in our random forest, we end up with trees that are not only trained on different sets of data (thanks to bagging) but also use different features to make decisions.

And that, my dear reader, creates uncorrelated trees that buffer and protect each other from their errors.

# 4. Create some Cooooooode

As described we will investigate the `adult` datasetz which contains information about peoles salary and personal details like education, race etc.

As this is no coding tutorial I will skip a detailed code explanaition. Just in short: I love `plotly.express` !! Thankt to this awesome library all of the displayed graphs are interactivea and you can therefor investigate the data on your own!

```python
# imports in any kind and shape

import pandas as pd
import plotly.express as px
import plotly.offline as off
import seaborn as sns

cols = ['workclass', 'fnlwgt', 'edu', 'edu-num', 'mariage_status',
          'occupation', 'relationship', 'race', 'sex', 'cap_gain',
           'cap_loss', 'h_per_week', 'native_country', 'salary']
data = pd.read_csv('adult.data', header=None, index_col=0, names=cols)
test = pd.read_csv('adult.test', header=None, index_col=0, names=cols)
```

The overall goal is to extract a reason for different salaries - as the most obvious (and falsy believed reason) is that working hours are the main reason for a big payday we will investigate this first: 

![Weekly hours per week](/images/dt-rf/race.html)

As wen can see there seems to a small trend that men tend to work a little bit longer than women, and the 50% quartile of the *white americans* seems to be a bit higher  than the 50% quartile of every other race.


